{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "78798ee3-8304-416a-82ad-f002b6aeee93",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2023-06-10T14:20:48.885818Z",
     "iopub.status.busy": "2023-06-10T14:20:48.885214Z",
     "iopub.status.idle": "2023-06-10T14:21:31.040028Z",
     "shell.execute_reply": "2023-06-10T14:21:31.039188Z",
     "shell.execute_reply.started": "2023-06-10T14:20:48.885790Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.9/dist-packages (1.22.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-addons in /usr/local/lib/python3.9/dist-packages (0.20.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons) (23.0)\n",
      "Requirement already satisfied: typeguard<3.0.0,>=2.7 in /usr/local/lib/python3.9/dist-packages (from tensorflow-addons) (2.13.3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.9/dist-packages (23.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (67.8.0)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.9/dist-packages (0.40.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /usr/local/lib/python3.9/dist-packages (3.4.1)\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.5.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.0.8)\n",
      "Collecting thinc<8.2.0,>=8.1.8 (from spacy)\n",
      "  Using cached thinc-8.1.10-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (924 kB)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.23.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy) (1.9.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.0.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy) (2.1.2)\n",
      "Installing collected packages: thinc, spacy\n",
      "  Attempting uninstall: thinc\n",
      "    Found existing installation: thinc 8.1.7\n",
      "    Uninstalling thinc-8.1.7:\n",
      "      Successfully uninstalled thinc-8.1.7\n",
      "  Attempting uninstall: spacy\n",
      "    Found existing installation: spacy 3.4.1\n",
      "    Uninstalling spacy-3.4.1:\n",
      "      Successfully uninstalled spacy-3.4.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "en-core-web-trf 3.4.1 requires spacy<3.5.0,>=3.4.1, but you have spacy 3.5.3 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed spacy-3.5.3 thinc-8.1.10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-trf==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_trf-3.5.0/en_core_web_trf-3.5.0-py3-none-any.whl (460.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 460.3/460.3 MB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-trf==3.5.0) (3.5.3)\n",
      "Collecting spacy-transformers<1.3.0,>=1.2.0.dev0 (from en-core-web-trf==3.5.0)\n",
      "  Downloading spacy_transformers-1.2.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 193.8/193.8 kB 32.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (8.1.10)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.4.5)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.4.2)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (4.64.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.23.4)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.9.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (67.8.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: transformers<4.30.0,>=3.4.0 in /usr/local/lib/python3.9/dist-packages (from spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (4.21.3)\n",
      "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.9/dist-packages (from spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (1.12.1+cu116)\n",
      "Requirement already satisfied: spacy-alignments<1.0.0,>=0.7.2 in /usr/local/lib/python3.9/dist-packages (from spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (0.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2019.11.28)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (3.9.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.9/dist-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (0.12.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (5.4.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.9/dist-packages (from transformers<4.30.0,>=3.4.0->spacy-transformers<1.3.0,>=1.2.0.dev0->en-core-web-trf==3.5.0) (0.12.1)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-trf==3.5.0) (2.1.2)\n",
      "Installing collected packages: spacy-transformers, en-core-web-trf\n",
      "  Attempting uninstall: spacy-transformers\n",
      "    Found existing installation: spacy-transformers 1.1.9\n",
      "    Uninstalling spacy-transformers-1.1.9:\n",
      "      Successfully uninstalled spacy-transformers-1.1.9\n",
      "  Attempting uninstall: en-core-web-trf\n",
      "    Found existing installation: en-core-web-trf 3.4.1\n",
      "    Uninstalling en-core-web-trf-3.4.1:\n",
      "      Successfully uninstalled en-core-web-trf-3.4.1\n",
      "Successfully installed en-core-web-trf-3.5.0 spacy-transformers-1.2.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_trf')\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "pip install PyMuPDF\n",
    "pip install tensorflow-addons\n",
    "pip install gensim\n",
    "pip install sent2vec\n",
    "pip install -U pip setuptools wheel\n",
    "pip install -U spacy\n",
    "python -m spacy download en_core_web_trf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eba7a9e-bae9-4d5c-8a9b-fbb7af2885fc",
   "metadata": {},
   "source": [
    "# Load libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc8171ba-d6a4-4eaf-914d-b4e07dd4dff5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz\n",
    "import random\n",
    "from pathlib import Path \n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          TFAutoModelForSequenceClassification,\n",
    "                          AutoTokenizer,\n",
    "                          TFAutoModel,\n",
    "                          TFDistilBertModel, \n",
    "                          TFDistilBertForSequenceClassification, \n",
    "                          DistilBertTokenizerFast, \n",
    "                          DistilBertConfig)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2bd671c4-98dd-4974-855c-d3350cb1824d",
   "metadata": {
    "tags": []
   },
   "source": [
    "df_labels = pd.read_csv(\"../data/oxml/raw_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "72d0d6a8-873d-4ec0-b06e-52f597a21ca5",
   "metadata": {
    "tags": []
   },
   "source": [
    "cat_to_code = dict(zip(set(df_labels[\"class\"]), range(len(set(df_labels[\"class\"])))))\n",
    "code_to_cat = dict(zip(range(len(set(df_labels[\"class\"]))), set(df_labels[\"class\"])))\n",
    "print(cat_to_code)\n",
    "print(\"-\"*20)\n",
    "print(code_to_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b39571-a533-43a5-aa74-7b6f43fddab5",
   "metadata": {},
   "source": [
    "# Helper functions for Modeling\n",
    "Some boilerplate to reduce rewriting code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29d4fc7b-8e09-4fcf-851c-1da35d185c65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenize_sets(train, val, test, tokenizer, max_seq_len):\n",
    "    \"\"\"tokenize and encode sequences the set splits\n",
    "    returns tokens dict from train, validation and test\n",
    "    \"\"\"\n",
    "    tokens_train = tokenizer.batch_encode_plus(\n",
    "        train.tolist(),\n",
    "        padding =True,\n",
    "        max_length = max_seq_len,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "    \n",
    "    tokens_val = tokenizer.batch_encode_plus(\n",
    "        val.tolist(),\n",
    "        padding =True,\n",
    "        max_length = max_seq_len,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "\n",
    "    tokens_test = tokenizer.batch_encode_plus(\n",
    "        test.tolist(),\n",
    "        padding =True,\n",
    "        max_length = max_seq_len,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=True\n",
    "    )\n",
    "    \n",
    "    return tokens_train, tokens_val, tokens_test\n",
    "\n",
    "def get_inputs(tokens_train, \n",
    "               train_labels, \n",
    "               tokens_val, \n",
    "               val_labels, \n",
    "               tokens_test, \n",
    "               test_labels):\n",
    "    \"\"\"Extracts input_ids and attention_mask \n",
    "    from tokenizer and turns them into numpy array\n",
    "    \"\"\"\n",
    "    # for train set\n",
    "    train_seq = np.array(tokens_train['input_ids'])\n",
    "    train_mask = np.array(tokens_train['attention_mask'])\n",
    "    train_y = np.array(train_labels)\n",
    "\n",
    "    # for validation set\n",
    "    val_seq = np.array(tokens_val['input_ids'])\n",
    "    val_mask = np.array(tokens_val['attention_mask'])\n",
    "    val_y = np.array(val_labels)\n",
    "\n",
    "    # for test set\n",
    "    test_seq = np.array(tokens_test['input_ids'])\n",
    "    test_mask = np.array(tokens_test['attention_mask'])\n",
    "    test_y = np.array(test_labels)\n",
    "    \n",
    "    return (train_seq, \n",
    "            train_mask, \n",
    "            train_y, \n",
    "            val_seq, \n",
    "            val_mask, \n",
    "            val_y, \n",
    "            test_seq, \n",
    "            test_mask, \n",
    "            test_y)\n",
    "\n",
    "def evaluate(y_true, y_pred, report=True, f1_average=\"micro\", target_names=None):\n",
    "    \"\"\"computes report and results from predicted labels\n",
    "    \"\"\"\n",
    "    if report==True:\n",
    "        print(f\"{classification_report(y_true, y_pred, target_names=target_names)}\\n\")\n",
    "        print(\"-\"*20)\n",
    "    print(f\"f1-score: {f1_score(y_true, y_pred, average=f1_average):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54668fe9-cf38-4610-aa78-2142e404c4f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def e_greddy_remover(x, prob_remove, prob):\n",
    "    \"\"\"random remove class labels from inputs with probability prob\n",
    "    \"\"\"\n",
    "    new = []\n",
    "    xs =  x.split(\" \")\n",
    "    for t in xs:\n",
    "        if t in prob_remove:\n",
    "            if random.random() >= prob:\n",
    "                new.append(t)\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            new.append(t)\n",
    "    return \" \".join(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f45bfd5-dcd7-4216-a376-263b80f2d86d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prep_text(df, col, new_col):\n",
    "    \"\"\" text processing function \n",
    "    \"\"\"\n",
    "    df[new_col] = df[col].apply(lambda x: x.lower())\n",
    "    df[new_col] = df[new_col].apply(lambda x: re.sub(r'(\\n+)(?=[a-zA-Z])', r' ', x))\n",
    "    df[new_col] = df[new_col].apply(lambda x: re.sub(r'(\\n+)(?=[0-9]+)', r' ', x))\n",
    "    df[new_col] = df[new_col].apply(lambda x: re.sub(r'(\\xa0+)(?=[a-zA-Z0-9])', r' ', x))\n",
    "    df[new_col] = df[new_col].apply(lambda x: x.replace(\"\\n\",\" \\n \"))\n",
    "    df[new_col] = df[new_col].apply(lambda x: re.sub(' +', ' ', x))\n",
    "    df[new_col] = df[new_col].apply(lambda x: re.sub('•', '', x))\n",
    "    df[new_col] = df[new_col].apply(lambda x: re.sub('-.', '.', x))\n",
    "    # df[new_col] = df[new_col].apply(lambda x: re.sub(r\"[^a-zA-Z0-9 ]\", r'', x))\n",
    "    df[new_col] = df[new_col].apply(lambda x: re.sub(r\"\\d+\", r'', x))\n",
    "    df[new_col] = df[new_col].apply(lambda x: re.sub(r\"\\s\\s+\", r' ', x))\n",
    "    df[new_col] = df[new_col].apply(lambda x: x.rstrip().lstrip())\n",
    "    return df\n",
    "\n",
    "\n",
    "def remove_words_with_probs(df, col, new_col, remove_words=[], prob = 0.25, strict_remove=True):\n",
    "    \"\"\" text processing function \n",
    "    \"\"\"\n",
    "    # delete remove words\n",
    "    if remove_words != []:\n",
    "        df[new_col] = df[col].apply(lambda x : e_greddy_remover(x, prob_remove=remove_words, prob=prob))\n",
    "        \n",
    "    if strict_remove:\n",
    "        # make it unsensitive about others the majority class\n",
    "        df[new_col] = df[new_col].apply(lambda x: \" \".join([x for x in x.split(\" \") if x not in [\"other\"]]))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70680ac7-2aba-4fb3-ae38-052bb1bd027d",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "abb6e91e-4eef-4343-9efb-63cf871d8bd8",
   "metadata": {
    "tags": []
   },
   "source": [
    "df_labels_prep = df_labels.copy()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "271e371f-9b26-4f09-932a-59186453542a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# remove labels from context as it creates leakage for classification\n",
    "remove_words = [\"environmental\", \"environment\", \"environmentally\"] +\\\n",
    "               [\"governance\", \"government\"] +\\\n",
    "               [\"social\", \"socially\"]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7836c5e9-ce52-4aef-a934-5751c004cbbf",
   "metadata": {
    "tags": []
   },
   "source": [
    "df_labels_prep = prep_text(df_labels, \n",
    "                           col=\"context\",\n",
    "                           new_col = \"context_prep\",)\n",
    "# df_labels_prep = remove_words_with_probs(df_labels_prep, \n",
    "#                            col=\"context\",\n",
    "#                            new_col = \"context_prep\",\n",
    "#                            remove_words=remove_words,\n",
    "#                            prob = 0.25,\n",
    "#                            strict_remove=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a046f1c9-5c5f-487a-9722-a3495933e1f0",
   "metadata": {},
   "source": [
    "# Chunking"
   ]
  },
  {
   "cell_type": "raw",
   "id": "96ee113c-33cb-433d-926a-9784d38dc8c7",
   "metadata": {},
   "source": [
    "df_labels_prep[\"context_prep_len\"] = df_labels_prep[\"context_prep\"].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70d114ba-6857-4198-9843-20c549ddfcbf",
   "metadata": {},
   "source": [
    "MAX_SEQ_LEN = 512\n",
    "prep_idxs = df_labels_prep[\"context_prep\"][df_labels_prep[\"context_prep_len\"] >= MAX_SEQ_LEN].index"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b18423a3-506c-4a6d-bc10-f133832ef548",
   "metadata": {},
   "source": [
    "prep_idxs = df_labels_prep[\"context_prep\"][df_labels_prep[\"context_prep_len\"] >= MAX_SEQ_LEN].index"
   ]
  },
  {
   "cell_type": "raw",
   "id": "329b73d7-cf12-46dc-8807-cdd0aa6beff0",
   "metadata": {},
   "source": [
    "df_to_chunk = df_labels_prep.iloc[prep_idxs]columns=)\n",
    "df_to_chunk.head(2)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0be26084-e666-411b-b0b3-2b6777b1fca3",
   "metadata": {},
   "source": [
    "nlp = spacy.load('en_core_web_trf') # Load the English Model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3d170d65-518e-4f9b-a5b2-138a00bb95d6",
   "metadata": {},
   "source": [
    "augmented_list = []\n",
    "wordcount_threshold = 15\n",
    "for s in df_to_chunk.iterrows():\n",
    "    if s[0]%2==0: print(s[0], end=\"\\r\")\n",
    "    doc = nlp(s[-1][\"context_prep\"])\n",
    "    for sent in doc.sents:\n",
    "        if len(sent) >= wordcount_threshold:\n",
    "            augmented_list.extend([s[-1][['id', 'class', 'page', 'fname', 'context']].tolist() + [sent.text] + [len(sent)]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c65df48f-303f-4d6a-9dfd-9544ac61d90f",
   "metadata": {},
   "source": [
    "df_augmented = pd.DataFrame(augmented_list, columns=df_labels_prep.columns)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b691268d-73ef-4f03-b08d-71f174347bcc",
   "metadata": {},
   "source": [
    "df_augmented[\"context_prep_len\"] = df_augmented[\"context_prep\"].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8e2a216-95b3-442e-b8b6-0d62fe64f389",
   "metadata": {},
   "source": [
    "df_augmented[df_augmented[\"context_prep_len\"]>15][\"class\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef572ef0-49d4-40ed-a8d2-8c37763eb2e7",
   "metadata": {},
   "source": [
    "df_augmented.to_csv(\"../data/oxml/augmented_chunks_1.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d0650c21-f714-48b5-92d3-508035b56337",
   "metadata": {},
   "source": [
    "df_not_augmented = df_labels_prep[~df_labels_prep.index.isin(prep_idxs)]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f7554627-9680-4711-ac0c-0cb5b03dc609",
   "metadata": {},
   "source": [
    "df_not_augmented.shape[0]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b609a3f6-2604-474a-b506-d05e7f002fc5",
   "metadata": {},
   "source": [
    "augmented_list = []\n",
    "wordcount_threshold = 15\n",
    "maxl = df_not_augmented.shape[0]\n",
    "for s in df_not_augmented.iterrows():\n",
    "    if s[0]%10==0: print(f\"{s[0]}\", end=\"\\r\")\n",
    "    doc = nlp(s[-1][\"context_prep\"])\n",
    "    for sent in doc.sents:\n",
    "        if len(sent) >= wordcount_threshold:\n",
    "            augmented_list.extend([s[-1][['id', 'class', 'page', 'fname', 'context']].tolist() + [sent.text] + [len(sent)]])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f7ab66e-fe67-4f46-926c-bb0aa451a703",
   "metadata": {},
   "source": [
    "df_not_augmented = pd.DataFrame(augmented_list, columns=df_labels_prep.columns)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc2b0619-c661-4a18-9d4b-bfe0a90b646a",
   "metadata": {},
   "source": [
    "df_not_augmented[\"context_prep_len\"] = df_not_augmented[\"context_prep\"].apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7d42b398-3611-44ba-b6cf-aad3f34523b1",
   "metadata": {},
   "source": [
    "df_not_augmented.to_csv(\"../data/oxml/augmented_chunks_2.csv\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b56dcbf4-f857-450d-aa22-e5857aff8599",
   "metadata": {},
   "source": [
    "df_not_augmented[df_not_augmented[\"context_prep_len\"]>15][\"class\"].value_counts(normalize=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ce4df789-fb95-4fc3-9a4f-6560a9c48706",
   "metadata": {},
   "source": [
    "df_final_aug = pd.concat([df_augmented, df_not_augmented], axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4aa7b6d8-cea2-4ec7-b008-6a9ea464e4d8",
   "metadata": {},
   "source": [
    "df_final_aug.to_csv(\"../data/oxml/augmented_chunks.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db54d6-5cd3-462c-982c-3da7998c17ae",
   "metadata": {},
   "source": [
    "# work imbalance"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c0caf3e-47a8-4324-adba-6eb00c36dff2",
   "metadata": {},
   "source": [
    "df_labels_prep.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dc6896-1864-4e8e-9fb5-8e4dfde73b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a898b72e-ba50-449d-bb6c-c588164dc819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e73ec8a-733e-479f-9614-ad0423ba67a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a960d151-b53d-4d1b-92ed-7bfed123797d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c25268-7181-4f26-9e67-ddb4c1afe1e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3cdcfc8-2cf0-4689-953e-0274e5f89267",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a147ac69-1fc0-4020-9047-76078301ce48",
   "metadata": {},
   "source": [
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f2e3e93c-af6b-416a-a48f-072d3ae4120a",
   "metadata": {},
   "source": [
    "df_labels_prep.groupby([\"class\",\"fname\"])[\"id\"].count()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d06d76ce-0d9b-4fc5-98fa-511bb312f93d",
   "metadata": {},
   "source": [
    "from itertools import permutations"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a48d6cfd-cb52-4e83-af3b-0289ff7bb3cb",
   "metadata": {},
   "source": [
    "res = []\n",
    "for a in list(permutations(set(df_labels_prep[\"class\"]), 2)):\n",
    "    common = set(df_labels_prep[\"fname\"][df_labels_prep[\"class\"]==a[0]]) \\\n",
    "    .intersection(set(df_labels_prep[\"fname\"][df_labels_prep[\"class\"]==a[-1]]))\n",
    "    if common != set():\n",
    "        res.append(common)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9feedd52-8a9f-4ba0-a417-4d65b6e81156",
   "metadata": {},
   "source": [
    "[len(x) for x in res]"
   ]
  },
  {
   "cell_type": "raw",
   "id": "af3213cf-b078-4e0c-a0fe-b7436c509cf2",
   "metadata": {},
   "source": [
    "for x in range(len(res)):\n",
    "    print(df_labels_prep[df_labels_prep[\"fname\"].isin(list(res[x]))].groupby([\"fname\"])[\"id\"].count().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d57f5f50-d892-4c4d-b160-9ceac07f06f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sets(X,y, seed=123):\n",
    "    train, temp, train_labels, temp_labels = train_test_split(X, y,\n",
    "                                                          shuffle=True,\n",
    "                                                          random_state=seed, \n",
    "                                                          test_size=0.3,\n",
    "                                                          stratify=y)\n",
    "\n",
    "    # we will use temp and temp to create validation and test set\n",
    "    val, test, val_labels, test_labels = train_test_split(temp, \n",
    "                                                          temp_labels,\n",
    "                                                          shuffle=True,\n",
    "                                                          random_state=seed, \n",
    "                                                          test_size=0.5,\n",
    "                                                          stratify=temp_labels)\n",
    "    \n",
    "    return train, train_labels, val, val_labels, test, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "17f35e81-f483-4fb6-aec5-db91c4e03fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_used = pd.read_csv(\"../data/oxml/augmented_chunks.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bc41b69a-f8d1-4432-9743-f244273f48ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_used = df_used[(df_used[\"context_prep_len\"]>=20) & (df_used[\"context_prep_len\"]<=256)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8cc10175-8f50-45ac-b459-4a328de7a47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'social': 0, 'environmental': 1, 'governance': 2, 'other': 3}\n",
      "--------------------\n",
      "{0: 'social', 1: 'environmental', 2: 'governance', 3: 'other'}\n"
     ]
    }
   ],
   "source": [
    "cat_to_code = dict(zip(set(df_used[\"class\"]), range(len(set(df_used[\"class\"])))))\n",
    "code_to_cat = dict(zip(range(len(set(df_used[\"class\"]))), set(df_used[\"class\"])))\n",
    "print(cat_to_code)\n",
    "print(\"-\"*20)\n",
    "print(code_to_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "153e1673-2368-432e-b5c8-6b274e2259d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_used[\"context_prep\"].copy()\n",
    "y = df_used[\"class\"].copy()\n",
    "\n",
    "y = y.replace(cat_to_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28851057-4ac0-4cde-8454-d3cc8ad1852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, train_labels, val, val_labels, test, test_labels = split_sets(X,y, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "cfa2a391-d0ba-475e-a2f5-50e09d6c6ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer_prep = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokens_train, tokens_val, tokens_test = tokenize_sets(train, val, test, base_tokenizer_prep, max_seq_len=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6e73a6ed-214a-4abe-a96b-cbedbe39664e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq, train_mask, train_y, val_seq, val_mask, val_y, test_seq, test_mask, test_y = get_inputs(tokens_train, \n",
    "                                                                                                   train_labels, \n",
    "                                                                                                   tokens_val, \n",
    "                                                                                                   val_labels, \n",
    "                                                                                                   tokens_test, \n",
    "                                                                                                   test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3fe147-95d8-4590-9b79-626bfcbf7f06",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6140faef-ba97-49bc-b742-1497d6680460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "89a9eecf-062b-4d99-a359-9b6f7152148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_set(model, seq, mask):\n",
    "    y_logits = model.predict([seq, mask])\n",
    "    return np.argmax(y_logits, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5718dcc8-72e9-4d57-b020-d31721c39577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_f1(y_true, y_pred):\n",
    "    def recall_m(y_true, y_pred):\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        Positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "        recall = TP / (Positives+K.epsilon())\n",
    "        return recall\n",
    "\n",
    "\n",
    "    def precision_m(y_true, y_pred):\n",
    "        TP = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        Pred_Positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "\n",
    "        precision = TP / (Pred_Positives+K.epsilon())\n",
    "        return precision\n",
    "\n",
    "    precision, recall = precision_m(y_true, y_pred), recall_m(y_true, y_pred)\n",
    "\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "da34a7f5-6d8f-4dde-bb35-e354e663f4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model( input_seq, input_mask,input_shape, model_name):\n",
    "    transformer_layer = TFAutoModel.from_pretrained(model_name)\n",
    "\n",
    "    \n",
    "    main_layer = transformer_layer(input_seq, attention_mask=input_mask, output_hidden_states=True)\n",
    "    \n",
    "    h = layers.Concatenate()(main_layer.hidden_states)\n",
    "    # h = layers.Dense(768, activation='relu',name = 'pre_classifier', \n",
    "    #                      kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.02))(h)\n",
    "    # h = layers.BatchNormalization()(main_layer.hidden_states[-1][:,0,:])\n",
    "    h = layers.Flatten()(h)\n",
    "    h = layers.BatchNormalization()(h)\n",
    "    h = layers.Dropout(0.2, name=\"dropout\")(h)\n",
    "    out = layers.Dense(NUM_LABELS, activation='linear',name = \"classifier\", \n",
    "                       kernel_initializer=tf.keras.initializers.TruncatedNormal(mean=0.0, stddev=0.02))(h)\n",
    "    \n",
    "    return tf.keras.Model(inputs=[input_seq,input_mask], outputs=out, name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "99b64d86-d4be-4b41-9e17-b7ba15013d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LABELS = len(set(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "dfe398f3-b239-4706-aa63-74b065f28282",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_shape = train_seq.shape[-1]\n",
    "inps = layers.Input(shape = (inp_shape,), dtype='int32')\n",
    "masks= layers.Input(shape = (inp_shape,), dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "58f6f1d1-8921-4f95-8ec0-ebc6922efe4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertModel: ['dropout_19', 'classifier', 'pre_classifier']\n",
      "- This IS expected if you are initializing TFDistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFDistilBertModel were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "model = build_model(input_seq = inps, \n",
    "                    input_mask = masks, \n",
    "                    input_shape=inp_shape, \n",
    "                    model_name = \"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "fbf59338-ebf7-4536-b411-b11ee6778b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 256)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_distil_bert_model (TFDistil  TFBaseModelOutput(l  66362880   ['input_1[0][0]',                \n",
      " BertModel)                     ast_hidden_state=(N               'input_2[0][0]']                \n",
      "                                one, 256, 768),                                                   \n",
      "                                 hidden_states=((No                                               \n",
      "                                ne, 256, 768),                                                    \n",
      "                                 (None, 256, 768),                                                \n",
      "                                 (None, 256, 768),                                                \n",
      "                                 (None, 256, 768),                                                \n",
      "                                 (None, 256, 768),                                                \n",
      "                                 (None, 256, 768),                                                \n",
      "                                 (None, 256, 768)),                                               \n",
      "                                 attentions=None)                                                 \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256, 5376)    0           ['tf_distil_bert_model[0][0]',   \n",
      "                                                                  'tf_distil_bert_model[0][1]',   \n",
      "                                                                  'tf_distil_bert_model[0][2]',   \n",
      "                                                                  'tf_distil_bert_model[0][3]',   \n",
      "                                                                  'tf_distil_bert_model[0][4]',   \n",
      "                                                                  'tf_distil_bert_model[0][5]',   \n",
      "                                                                  'tf_distil_bert_model[0][6]']   \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 1376256)      0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 1376256)     5505024     ['flatten[0][0]']                \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 1376256)      0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " classifier (Dense)             (None, 4)            5505028     ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 77,372,932\n",
      "Trainable params: 74,620,420\n",
      "Non-trainable params: 2,752,512\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bf76deaf-8d3c-4b81-8542-065697452045",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_sample_weight, compute_class_weight\n",
    "sample_weights = compute_sample_weight(class_weight=\"balanced\",y=train_labels)\n",
    "class_weights = compute_class_weight(class_weight=\"balanced\",classes = np.unique(train_labels), y=train_labels)\n",
    "class_weights = dict([x for x in zip(range(NUM_LABELS),class_weights)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24ccda06-ed32-403f-b5bb-fc1b9cce8b34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 1.2529122679286495,\n",
       " 1: 1.6381485007139458,\n",
       " 2: 2.1115030674846627,\n",
       " 3: 0.4721841130470572}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "96e0718e-b934-4175-864a-c90b0392b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, \n",
    "              loss=loss,\n",
    "              metrics=['accuracy', \n",
    "                       tfa.metrics.F1Score(num_classes=4, \n",
    "                                           average='micro',\n",
    "                                            threshold=0.5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "87e13d32-b3ed-4107-badb-293583bd9148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "574/574 [==============================] - 155s 264ms/step - loss: 4.8391 - accuracy: 0.8013 - f1_score: 0.6938 - val_loss: 5.6054 - val_accuracy: 0.7936 - val_f1_score: 0.6430\n"
     ]
    }
   ],
   "source": [
    "# train the model \n",
    "BATCH_SIZE = 24\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\n",
    "history = model.fit([train_seq, train_mask],\n",
    "                    train_y,\n",
    "                    epochs = 1,\n",
    "                    shuffle=True,\n",
    "                    batch_size = BATCH_SIZE,\n",
    "                    # class_weight = class_weights,\n",
    "                    # sample_weight = sample_weights,\n",
    "                    validation_data = ([val_seq, val_mask],val_y),\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "6e3c9fa5-3d6a-4135-abcf-139dafd977fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 10s 96ms/step\n"
     ]
    }
   ],
   "source": [
    "# y_pred_train = predict_set(model, train_seq, train_mask)\n",
    "# y_pred_valid = predict_set(model, val_seq, val_mask)\n",
    "y_pred_test = predict_set(model, test_seq, test_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "545e2481-a164-47b4-83e2-c5388aa16391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "       social       0.73      0.65      0.69       589\n",
      "environmental       0.71      0.74      0.72       451\n",
      "   governance       0.56      0.68      0.62       349\n",
      "        other       0.91      0.89      0.90      1562\n",
      "\n",
      "     accuracy                           0.80      2951\n",
      "    macro avg       0.73      0.74      0.73      2951\n",
      " weighted avg       0.80      0.80      0.80      2951\n",
      "\n",
      "\n",
      "--------------------\n",
      "f1-score: 0.7974\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate(train_labels, y_pred_train, target_names=cat_to_code)\n",
    "# print(\"-\"*20)\n",
    "# evaluate(val_labels, y_pred_valid, target_names=cat_to_code)\n",
    "print(\"-\"*20)\n",
    "evaluate(test_labels, y_pred_test, target_names=cat_to_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9018f04c-3be6-4692-bc68-1a273cf60a12",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2489e8fd-ea2d-40b2-8c47-51d053e8f015",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_single_set(tokenizer, data, max_seq_len):\n",
    "    # tokenize and encode sequences in the validation set\n",
    "    tokens = tokenizer.batch_encode_plus(\n",
    "        data.tolist(),\n",
    "        padding =True,\n",
    "        max_length = max_seq_len,\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False\n",
    "    )\n",
    "    return tokens\n",
    "\n",
    "def get_single_inputs(tokens, labels):\n",
    "    # returns tokens seq, masks and labels in numpy array for single set\n",
    "    seq = np.array(tokens['input_ids'])\n",
    "    mask = np.array(tokens['attention_mask'])\n",
    "    label = np.array(labels)\n",
    "    return seq, mask, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bdfd4f82-d827-4f28-97e0-8a9ae438eed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfsub = pd.read_csv(\"../data/oxml/submission_set.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9ce0b1a8-c907-449b-9bf3-ee1d157e9050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 1s 93ms/step\n"
     ]
    }
   ],
   "source": [
    "sub_labels_prep = prep_text(dfsub, col=\"context\", new_col = \"context_prep\")\n",
    "sub_tokens = tokenize_single_set(base_tokenizer_prep, \n",
    "                                     sub_labels_prep[\"context_prep\"], \n",
    "                                     max_seq_len = 256)\n",
    "\n",
    "sub_labels = sub_labels_prep[\"class\"].replace(cat_to_code)\n",
    "sub_seq, sub_mask, sub_y = get_single_inputs(sub_tokens, sub_labels)\n",
    "y_pred =  predict_set(model, sub_seq, sub_mask)\n",
    "dfsub[\"class\"] = pd.Series(y_pred).replace(code_to_cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3ecda314-6f31-4b5c-80e9-f81af6303948",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = dfsub[[\"id\",\"class\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fc3bde8f-5a82-49a9-895f-181f5ec84e4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "social           161\n",
       "other            153\n",
       "environmental     82\n",
       "governance        70\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_submission[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "787cbed8-c389-4ca9-abfc-aab31938c355",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission.to_csv(\"../data/oxml/submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
