{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4877ec23-b5f9-470c-a632-408d19ac28b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install transformers timm evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "482783c0-0105-4d3e-8b46-980309aa39fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path \n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "import evaluate\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoImageProcessor, TableTransformerForObjectDetection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7d4b65b-afa4-47a9-9059-dde784f5f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = Path(\"kaggle/input/oxml-2023-x-ml-cases-table-detector/task2/data/train/\")\n",
    "TRAIN_IMAGES_DIR = TRAIN_DIR / \"images/\"\n",
    "TRAIN_LABELS_DIR = TRAIN_DIR / \"labels/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a11906ad-15bf-4950-bfe8-9742d795ade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(label_file): \n",
    "    with open(Path(TRAIN_LABELS_DIR) / label_file) as file: \n",
    "        return file.readlines()\n",
    "    \n",
    "\n",
    "def draw_bounding_box(doc_prefix):\n",
    "    \"\"\"utils function that returns document image along with the bouding box\"\"\"\n",
    "    image_file = doc_prefix+\".jpg\"\n",
    "    img = cv2.imread(str(TRAIN_IMAGES_DIR / image_file))\n",
    "    dh, dw, _ = img.shape\n",
    "    \n",
    "    coordinates = get_labels(doc_prefix+\".txt\")\n",
    "    \n",
    "    for bounding_box in coordinates:\n",
    "        _, x, y, width, height = map(float, bounding_box.split(' '))\n",
    "\n",
    "        left = int((x - width / 2) * dw)\n",
    "        right = int((x + width / 2) * dw)\n",
    "        top = int((y - height / 2) * dh)\n",
    "        bottom = int((y + height / 2) * dh)\n",
    "\n",
    "        cv2.rectangle(img, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "        # Convert the image back to PIL format\n",
    "        image_with_bbox = Image.fromarray(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "    return image_with_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2355361-3844-419a-95e8-056687d5bad7",
   "metadata": {},
   "source": [
    "# 1. Transformer Zero-Shot Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5b08218-1d59-4c1f-92e1-e75e5601e73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForObjectDetection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2de8640d-4999-4c8a-972f-ec1f340cda62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import DictWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89c3006b-4aa9-4ccf-a023-0de25390d5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directories\n",
    "DATA_DIR = \"kaggle/input/oxml-2023-x-ml-cases-table-detector/task2/data\"\n",
    "TEST_DIR = \"test\"\n",
    "TRAIN_DIR = \"train\"\n",
    "IMG_DIR = \"images\"\n",
    "LABEL_DIR = \"labels\"\n",
    "SUB_FILE = \"kaggle/input/oxml-2023-x-ml-cases-table-detector/task2/submission_sample.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0921340-4be8-48a3-ab70-4f1d4448efdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(filepath):\n",
    "    files = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(filepath):\n",
    "        files.extend(filenames)\n",
    "        break\n",
    "    return files\n",
    "filenames = get_filenames(os.path.join(DATA_DIR, TEST_DIR, IMG_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17f20437-544b-4696-b24a-abc973e639e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_img_files = [os.path.join(DATA_DIR, TEST_DIR, IMG_DIR, f) for f in filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8cd3ab9-4001-43f2-8742-57b5aa0ca186",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n",
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    }
   ],
   "source": [
    "modelname = \"microsoft/table-transformer-detection\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(modelname)\n",
    "model = AutoModelForObjectDetection.from_pretrained(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae2e9a98-2ba0-4139-8cf6-aa6bb9abefa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.70%\r"
     ]
    }
   ],
   "source": [
    "sub_results = []\n",
    "for i, img_path in enumerate(all_img_files):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    print(f\"{(i+1)/len(all_img_files):.2%}\", end='\\r')\n",
    "    tensor_input = image_processor(images=img, return_tensors=\"pt\")\n",
    "    output_tensor = model(**tensor_input) # outputs (center_x, center_y, width, height)\n",
    "    img_size = img.size # is (width, height) fmt\n",
    "    target_size = torch.tensor([img_size[::-1]]) # must be (height, width) fmt\n",
    "    # post_process output format :: (top_left_x, top_left_y, bottom_right_x, bottom_right_y) format\n",
    "    results = image_processor.post_process_object_detection(output_tensor,\n",
    "                                                                threshold=0.9,\n",
    "                                                                target_sizes=target_size)[0]\n",
    "    sub_results.append((results, img_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf80408f-0029-41d6-ae42-c7334486088e",
   "metadata": {},
   "source": [
    "# Process Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abe49961-01b4-4d07-a98f-4c68435dbe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sub_format(c):\n",
    "    \"\"\"\n",
    "    changes: top_left_x, top_left_y, bottom_right_x, bottom_right_y\n",
    "    to: center_x, center_y, width, height\n",
    "    \"\"\"\n",
    "    return list(\n",
    "        ((c[0]+(c[2]-c[0])/2)/c[4], \n",
    "        (c[1]+(c[3]-c[1])/2)/c[5],\n",
    "        (c[2]-c[0])/c[4],\n",
    "        (c[3]-c[1])/c[5])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19c8ad28-5c6f-4009-a24b-2873e530b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = []\n",
    "for f, i in zip(filenames, sub_results):\n",
    "    if len(i[0][\"boxes\"]) != 0:  \n",
    "        coord = i[0][\"boxes\"].detach().numpy().flatten().tolist()+ list(i[-1])\n",
    "        coord = to_sub_format(coord)\n",
    "    else:\n",
    "        coord = [0]*4\n",
    "    \n",
    "    coords.append(coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f321efcf-f5a2-4068-b532-bd179c4c669f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub_coords = pd.DataFrame(coords, columns=['x', 'y', 'width', 'height'])\n",
    "df_sub_coords[\"doc_id\"] = filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "de9a9c32-84fe-4735-9fbf-da254d324f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_submission = pd.read_csv(SUB_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "eafa23d3-07c3-4df9-b336-3464fa87e966",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_sub_coords.copy()\n",
    "df_final = df_submission[[\"doc_id\"]].merge(df_final, on=\"doc_id\", how=\"left\")\n",
    "df_final = df_final.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fcb0fc4-2a18-408b-8a6b-d064773c20d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>width</th>\n",
       "      <th>height</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.501449</td>\n",
       "      <td>0.489871</td>\n",
       "      <td>0.766514</td>\n",
       "      <td>0.704771</td>\n",
       "      <td>doc_45_10.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.503713</td>\n",
       "      <td>0.385183</td>\n",
       "      <td>0.533592</td>\n",
       "      <td>0.419413</td>\n",
       "      <td>doc_45_7.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.166678</td>\n",
       "      <td>0.671626</td>\n",
       "      <td>8.311664</td>\n",
       "      <td>1.004202</td>\n",
       "      <td>doc_43_42.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>doc_36_27.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.526616</td>\n",
       "      <td>0.552478</td>\n",
       "      <td>0.772723</td>\n",
       "      <td>0.211361</td>\n",
       "      <td>doc_42_68.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          x         y     width    height         doc_id\n",
       "0  0.501449  0.489871  0.766514  0.704771  doc_45_10.jpg\n",
       "1  0.503713  0.385183  0.533592  0.419413   doc_45_7.jpg\n",
       "2  5.166678  0.671626  8.311664  1.004202  doc_43_42.jpg\n",
       "3  0.000000  0.000000  0.000000  0.000000  doc_36_27.jpg\n",
       "4  0.526616  0.552478  0.772723  0.211361  doc_42_68.jpg"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3265aa9e-9fc9-4229-b21f-cf638681bf9c",
   "metadata": {},
   "source": [
    "# Save submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556c03fe-2a73-4c96-820e-33d5285e3dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv(f\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
